# =============================================================================
# HE-300 Ethics Benchmark Suite - GitHub Actions
# =============================================================================
#
# Comprehensive benchmark suite for evaluating LLM ethical reasoning across:
# - Different hardware configurations (CPU, GPU, Apple Silicon)
# - Different inference engines (Ollama, vLLM, llama.cpp, OpenAI, Anthropic)
# - Different quantization levels (fp16, int8, int4)
# - Different model weights
#
# Run this workflow on self-hosted runners with different hardware to compare
# performance and accuracy across your infrastructure.
#
# =============================================================================

name: HE-300 Benchmark

on:
  push:
    branches: [main]
    paths:
      - 'tests/**'
      - 'scripts/**'
      - 'docker/**'
      - '.github/workflows/he300-benchmark.yml'
  workflow_dispatch:
    inputs:
      model:
        description: 'LLM Model (e.g., ollama/llama3.2, openai/gpt-4o-mini)'
        required: true
        default: 'ollama/llama3.2'
        type: string
      quantization:
        description: 'Quantization level'
        required: true
        default: 'default'
        type: choice
        options: ['default', 'fp16', 'int8', 'int4', 'q4_0', 'q4_k_m', 'q5_k_m', 'q8_0']
      sample_size:
        description: 'Number of scenarios to evaluate'
        required: true
        default: '50'
        type: choice
        options: ['10', '30', '50', '100', '300']
      inference_engine:
        description: 'Inference engine'
        required: true
        default: 'ollama'
        type: choice
        options: ['ollama', 'vllm', 'llamacpp', 'openai', 'anthropic', 'mock']
      runner:
        description: 'Runner to use'
        required: true
        default: 'ubuntu-latest'
        type: choice
        options: ['ubuntu-latest', 'macos-latest', 'self-hosted-gpu', 'self-hosted-cpu']

env:
  PYTHON_VERSION: '3.11'
  CIRISNODE_REPO: 'rng-ops/CIRISNode'
  CIRISNODE_BRANCH: 'feature/eee-integration'
  EEE_REPO: 'rng-ops/ethicsengine_enterprise'
  EEE_BRANCH: 'feature/he300-api'

jobs:
  # ---------------------------------------------------------------------------
  # Unit Tests - Component Validation
  # ---------------------------------------------------------------------------
  test-eee-api:
    name: "ðŸ§ª EthicsEngine HE-300 API"
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout EthicsEngine
        uses: actions/checkout@v4
        with:
          repository: ${{ env.EEE_REPO }}
          ref: ${{ env.EEE_BRANCH }}
          
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-cov httpx
          
      - name: Run HE-300 API tests
        env:
          FF_MOCK_LLM: "true"
          HE300_ENABLED: "true"
        run: |
          pytest tests/test_he300_api.py -v --tb=short -x
          
      - name: Summary
        if: always()
        run: |
          echo "## ðŸ§ª EthicsEngine HE-300 API Tests" >> $GITHUB_STEP_SUMMARY
          echo "Validates /he300/batch, /he300/catalog, /he300/health endpoints" >> $GITHUB_STEP_SUMMARY

  test-cirisnode:
    name: "ðŸ§ª CIRISNode EEE Client"
    runs-on: ubuntu-latest
    
    services:
      redis:
        image: redis:7-alpine
        ports: ['6379:6379']
        options: --health-cmd "redis-cli ping" --health-interval 10s --health-timeout 5s --health-retries 5
    
    steps:
      - name: Checkout CIRISNode
        uses: actions/checkout@v4
        with:
          repository: ${{ env.CIRISNODE_REPO }}
          ref: ${{ env.CIRISNODE_BRANCH }}
          
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-cov httpx respx
          
      - name: Run EEE integration tests
        env:
          REDIS_URL: redis://localhost:6379/0
          JWT_SECRET: test-secret
          EEE_ENABLED: "false"
        run: |
          pytest tests/test_he300_integration.py tests/test_celery_he300.py -v --tb=short -x
          
      - name: Summary
        if: always()
        run: |
          echo "## ðŸ§ª CIRISNode EEE Client Tests" >> $GITHUB_STEP_SUMMARY
          echo "Validates EEEClient and Celery HE-300 benchmark task" >> $GITHUB_STEP_SUMMARY

  # ---------------------------------------------------------------------------
  # Full Benchmark Run
  # ---------------------------------------------------------------------------
  benchmark:
    name: "ðŸ“Š HE-300 Full Benchmark"
    runs-on: ${{ github.event.inputs.runner || 'ubuntu-latest' }}
    needs: [test-eee-api, test-cirisnode]
    if: github.event_name == 'workflow_dispatch'
    
    env:
      MODEL: ${{ github.event.inputs.model || 'ollama/llama3.2' }}
      QUANTIZATION: ${{ github.event.inputs.quantization || 'default' }}
      SAMPLE_SIZE: ${{ github.event.inputs.sample_size || '50' }}
      INFERENCE_ENGINE: ${{ github.event.inputs.inference_engine || 'ollama' }}
    
    steps:
      - name: Checkout integration repo
        uses: actions/checkout@v4
        with:
          repository: 'rng-ops/he300-integration'
          
      - name: Checkout CIRISNode
        uses: actions/checkout@v4
        with:
          repository: ${{ env.CIRISNODE_REPO }}
          ref: ${{ env.CIRISNODE_BRANCH }}
          path: submodules/cirisnode
          
      - name: Checkout EthicsEngine
        uses: actions/checkout@v4
        with:
          repository: ${{ env.EEE_REPO }}
          ref: ${{ env.EEE_BRANCH }}
          path: submodules/ethicsengine
          
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Collect system info
        id: sysinfo
        run: |
          echo "## ðŸ–¥ï¸ System Information" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Property | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Runner | ${{ runner.os }} / ${{ runner.arch }} |" >> $GITHUB_STEP_SUMMARY
          echo "| CPU | $(nproc) cores |" >> $GITHUB_STEP_SUMMARY
          echo "| Memory | $(free -h | awk '/^Mem:/ {print $2}') |" >> $GITHUB_STEP_SUMMARY
          
          # Check for GPU
          if command -v nvidia-smi &> /dev/null; then
            GPU_INFO=$(nvidia-smi --query-gpu=name,memory.total --format=csv,noheader | head -1)
            echo "| GPU | $GPU_INFO |" >> $GITHUB_STEP_SUMMARY
            echo "gpu_available=true" >> $GITHUB_OUTPUT
          else
            echo "| GPU | None detected |" >> $GITHUB_STEP_SUMMARY
            echo "gpu_available=false" >> $GITHUB_OUTPUT
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Benchmark Configuration" >> $GITHUB_STEP_SUMMARY
          echo "| Setting | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|---------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Model | ${{ env.MODEL }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Quantization | ${{ env.QUANTIZATION }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Sample Size | ${{ env.SAMPLE_SIZE }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Inference Engine | ${{ env.INFERENCE_ENGINE }} |" >> $GITHUB_STEP_SUMMARY
          
      - name: Start Ollama (if needed)
        if: env.INFERENCE_ENGINE == 'ollama'
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          ollama serve &
          sleep 5
          
          # Pull model
          MODEL_NAME="${{ env.MODEL }}"
          MODEL_NAME="${MODEL_NAME#ollama/}"
          echo "Pulling model: $MODEL_NAME"
          ollama pull "$MODEL_NAME"
          
      - name: Start services
        run: |
          docker compose -f docker/docker-compose.test.yml up -d
          
          # Wait for services
          echo "Waiting for services..."
          timeout 120 bash -c 'until curl -sf http://localhost:18000/health 2>/dev/null; do sleep 5; done' || true
          timeout 120 bash -c 'until curl -sf http://localhost:18080/health 2>/dev/null; do sleep 5; done' || true
          
      - name: Install benchmark dependencies
        run: |
          pip install httpx pyyaml aiohttp tqdm
          
      - name: Run HE-300 Benchmark
        id: benchmark
        run: |
          mkdir -p results
          
          python3 << 'BENCHMARK_SCRIPT'
          import asyncio
          import httpx
          import json
          import time
          import os
          from datetime import datetime
          
          CIRISNODE_URL = "http://localhost:18000"
          EEE_URL = "http://localhost:18080"
          SAMPLE_SIZE = int(os.environ.get("SAMPLE_SIZE", "50"))
          MODEL = os.environ.get("MODEL", "ollama/llama3.2")
          
          async def run_benchmark():
              results = {
                  "timestamp": datetime.utcnow().isoformat(),
                  "model": MODEL,
                  "sample_size": SAMPLE_SIZE,
                  "quantization": os.environ.get("QUANTIZATION", "default"),
                  "inference_engine": os.environ.get("INFERENCE_ENGINE", "ollama"),
                  "runner": os.environ.get("RUNNER_NAME", "unknown"),
                  "scenarios": [],
                  "summary": {}
              }
              
              async with httpx.AsyncClient(timeout=300) as client:
                  # Get catalog
                  try:
                      catalog_resp = await client.get(f"{EEE_URL}/he300/catalog")
                      catalog = catalog_resp.json()
                      scenarios = catalog.get('scenarios', catalog.get('pipelines', []))
                      print(f"Catalog: {len(scenarios)} scenarios available")
                  except Exception as e:
                      print(f"Could not fetch catalog: {e}")
                      catalog = {}
                      scenarios = []
                  
                  # Run batch evaluation (mock for now)
                  start_time = time.time()
                  
                  # Get scenario IDs
                  scenario_ids = [s.get('id', s) if isinstance(s, dict) else s for s in scenarios[:SAMPLE_SIZE]]
                  
                  try:
                      batch_resp = await client.post(
                          f"{EEE_URL}/he300/batch",
                          json={
                              "scenario_ids": scenario_ids,
                              "model": MODEL,
                              "options": {"mock": True}
                          }
                      )
                      batch_result = batch_resp.json()
                  except Exception as e:
                      print(f"Batch evaluation error: {e}")
                      batch_result = {"results": [], "error": str(e)}
                  
                  elapsed = time.time() - start_time
                  
                  # Calculate summary
                  total = len(batch_result.get("results", []))
                  correct = sum(1 for r in batch_result.get("results", []) if r.get("correct", False))
                  
                  results["summary"] = {
                      "total": total,
                      "correct": correct,
                      "accuracy": correct / total if total > 0 else 0,
                      "elapsed_seconds": elapsed,
                      "scenarios_per_second": total / elapsed if elapsed > 0 else 0
                  }
                  
                  results["scenarios"] = batch_result.get("results", [])
              
              # Save results
              output_file = f"results/he300-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}.json"
              with open(output_file, "w") as f:
                  json.dump(results, f, indent=2)
              
              print(f"\n=== HE-300 Benchmark Results ===")
              print(f"Model: {MODEL}")
              print(f"Scenarios: {results['summary']['total']}")
              print(f"Correct: {results['summary']['correct']}")
              print(f"Accuracy: {results['summary']['accuracy']*100:.2f}%")
              print(f"Time: {elapsed:.2f}s")
              print(f"Throughput: {results['summary']['scenarios_per_second']:.2f} scenarios/sec")
              print(f"Results saved to: {output_file}")
              
              return results
          
          asyncio.run(run_benchmark())
          BENCHMARK_SCRIPT
          
      - name: Generate benchmark report
        run: |
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## ðŸ“Š Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Parse latest results
          LATEST=$(ls -t results/*.json | head -1)
          if [ -f "$LATEST" ]; then
            python3 << REPORT_SCRIPT
          import json
          with open("$LATEST") as f:
              r = json.load(f)
          s = r.get("summary", {})
          print(f"| Metric | Value |")
          print(f"|--------|-------|")
          print(f"| Total Scenarios | {s.get('total', 0)} |")
          print(f"| Correct | {s.get('correct', 0)} |")
          print(f"| **Accuracy** | **{s.get('accuracy', 0)*100:.2f}%** |")
          print(f"| Elapsed Time | {s.get('elapsed_seconds', 0):.2f}s |")
          print(f"| Throughput | {s.get('scenarios_per_second', 0):.2f} scenarios/sec |")
          REPORT_SCRIPT
          fi >> $GITHUB_STEP_SUMMARY
          
      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: he300-results-${{ github.run_number }}
          path: results/
          retention-days: 90
          
      - name: Cleanup
        if: always()
        run: |
          docker compose -f docker/docker-compose.test.yml down -v || true

  # ---------------------------------------------------------------------------
  # Quick Smoke Test (runs on every push to main)
  # ---------------------------------------------------------------------------
  smoke-test:
    name: "ðŸ”¥ Quick Smoke Test"
    runs-on: ubuntu-latest
    needs: [test-eee-api, test-cirisnode]
    if: github.event_name == 'push'
    
    steps:
      - name: Checkout integration repo
        uses: actions/checkout@v4
        with:
          repository: 'rng-ops/he300-integration'
          
      - name: Checkout repos
        run: |
          git clone --depth 1 --branch ${{ env.CIRISNODE_BRANCH }} https://github.com/${{ env.CIRISNODE_REPO }}.git submodules/cirisnode
          git clone --depth 1 --branch ${{ env.EEE_BRANCH }} https://github.com/${{ env.EEE_REPO }}.git submodules/ethicsengine
          
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Quick validation
        run: |
          echo "## ðŸ”¥ Smoke Test" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Check | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          
          # Check files exist
          if [ -f "submodules/ethicsengine/api/routers/he300.py" ]; then
            echo "| EEE HE-300 Router | âœ… |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| EEE HE-300 Router | âŒ |" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ -f "submodules/cirisnode/cirisnode/utils/eee_client.py" ]; then
            echo "| CIRISNode EEE Client | âœ… |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| CIRISNode EEE Client | âŒ |" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ -f "submodules/ethicsengine/data/pipelines/he300/manifest.json" ]; then
            SCENARIO_COUNT=$(python3 -c "import json; m=json.load(open('submodules/ethicsengine/data/pipelines/he300/manifest.json')); print(m.get('total_scenarios', len(m.get('scenarios', []))))" 2>/dev/null || echo "?")
            echo "| HE-300 Scenarios | âœ… ($SCENARIO_COUNT) |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| HE-300 Scenarios | âš ï¸ Not found |" >> $GITHUB_STEP_SUMMARY
          fi
          
      - name: Summary
        run: |
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Next Steps" >> $GITHUB_STEP_SUMMARY
          echo "To run a full benchmark, use **Run workflow** and select your configuration." >> $GITHUB_STEP_SUMMARY
